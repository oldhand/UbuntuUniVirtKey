# 接口流程概述

- 接口入口：https://github.com/kube-stack/sdsctl/blob/main/cmd/sdsctl/main.go

## 存储池相关

> 存储池创建

- 判断type是否合法：本地、cephfs、cephrbd
- 判断url路径是否存在，不存在则创建目录
- 对于不同类型的存储池
  - 本地
  - cephfs：从/etc/ceph/keyring路径下获取ceph secret，挂载url路径（容器+宿主机）
    - 后续操作和本地文件路径一样
  - cephrbd：创建cephblockpools并等待ready
- define & start存储池
  - 原生支持rbd类型，提供如下参数即可：
    - "source-host": "10.109.253.11:6789",
    - "source-name": "rbdpool"
    - "source-path": url路径
- 根据auto-start参数设置自动启动
- 在url路径下创建content文件，包含存储池的类型（vmd、vmdi等）
- 更新vmp spec参数

> 存储池删除

- 根据name查找对应pool对象，destroy & undefine
- 删除vmp

> 存储池自动启动

- 根据name查找对应pool对象，并设置自动启动
- 更新pool.autostart参数为CRD_Pool_Active

> 存储池启动

- 根据name查找对应pool对象，并启动
- 更新pool.state参数为CRD_Pool_Active

> 存储池停止

- 根据name查找对应pool对象，并停止
- 更新pool.state参数为CRD_Pool_Inactive

## nfs镜像存储池

说明：只存在单一nfs服务，多个nfs存储池本质上是多个nfs前缀路径

> 前置工作

- 在环境初始化阶段，已经完成如下资源创建：
  - cephnfs：https://github.com/kube-stack/sdsctl/blob/main/rook/nfs/nfs.yaml
  - object store：https://github.com/kube-stack/sdsctl/blob/main/rook/cephrgw/object.yaml

> 创建流程

- 等待cephnfs池创建完毕：`phase=Ready`
- 根据名称name，创建cephfs的subvolumegroup，并export nfs路径
- 获取nfs service ip，并根据本地路径local-path完成挂载：容器+宿主机
- 更新vmp spec参数

> 删除流程

- 根据名称name，删除export nfs路径
  - 未删除cephfs的subvolumegroup
- 取消挂载：容器+宿主机
- 删除vmp

## cephrgw镜像存储池

说明：cephrgw只创建一个默认bucket

- 在环境初始化阶段，已经完成如下资源创建：
  - cephnfs：https://github.com/kube-stack/sdsctl/blob/main/rook/nfs/nfs.yaml
  - object store：https://github.com/kube-stack/sdsctl/blob/main/rook/cephrgw/object.yaml

> 创建流程

- 创建storageclass
- 创建obc（obc bound后会创建ob及configmap、secret等资源）
- 等待configmap的相关资源创建完毕，并获取：bucket名称、host、ip、port
- 等待secret的相关资源创建完毕，并获取：access-id、access-key
- 更新vmp spec参数

> 删除流程

- 删除obc
- 等待obc删除完成后，再删除storageclass（否则obc无法完成删除）
- 删除vmp

## 磁盘相关

> 创建

- 判断存储池是否active
- 判断disk是否已经存在
- 创建disk
  - 每个disk都包含在存储池路径下一个独立文件夹下
  - 创建config.json文件，包含disk名称、pool名称、disk路径以及当前disk指向的磁盘文件（为快照服务）
- 更新vmd spec参数

> 删除

- 判断存储池是否active
- 判断disk是否已经存在
- 删除disk文件夹
- 删除vmd

> 调整大小

- 判断存储池是否active
- 判断disk是否已经存在
- 调用接口调整磁盘大小，如果磁盘变小需要添加--shrink参数
- 更新volume.capacity参数

> 克隆

- 判断新克隆的pool是否是active
- 判断旧克隆的pool是否active、disk是否存在
- 创建middle目录（克隆完成后删除），从源头复制磁盘
- 如果复制的磁盘存在backfile，则rebase使得新克隆的磁盘无backfile
- 比较旧克隆和新克隆磁盘所在节点名称
  - 在同一节点：rename middle目录文件名结课
  - 在不同节点：获取新克隆的磁盘所在节点ip，scp到新节点
- 为新克隆的disk创建vmd

> 从镜像创建

- 判断存储池是否active
- 判断disk是否已经存在
- 判断是否似乎full-copy
  - 是full-copy：格式转为qcow2（qcow2才支持rebase），rebase磁盘为无backfile
  - 不是full-copy：基于image作为backfile，创建qcow2类型的disk

- 更新vmd spec参数

## 磁盘镜像相关

> 创建磁盘镜像（从文件路径）

- 判断源文件路径是否存在
- 判断目标image文件夹是否存在，不存在则创建
- 复制disk为image，并将image rebase为自身（无backfile
- image文件夹创建config文件记录
- 创建vmdi

> 创建磁盘镜像（从pool+disk）

- 判断新旧pool是否active
- 判断pool content是否是vmdi
- 调用`创建磁盘镜像（从文件路径）`相同方法

> 删除磁盘镜像

- 判断存储池是否active
- 判断pool content是否是vmdi
- 判断disk是否存在
- 删除image对应的文件夹
- 删除 vmdi

> 上传

- 判断存储池是否active
- 判断pool content是否是vmdi
- 判断镜像hub类型
  - cephrgw：基于aws-sdk，将对应路径下的image对象上传到bucket
  - nfs：按nfs共享路径，复制文件即可



> 下载

- 判断存储池是否active
- 判断pool content是否是vmdi
- 判断镜像hub类型
  - cephrgw：基于aws-sdk，从bucket中下载对应路径下的image对象
  - nfs：按nfs共享路径，复制文件即可
- 创建vmdi

## 外部快照相关

> 创建

- 判断存储池是否active
- 判断snapshot是否已经存在
- 解析config，判断current disk是否存在
- 判断/snapshots文件夹是否存在（每个external snapshot都存在于/snapshots下一个单独文件夹中）
- 判断domain参数
  - 非空
    - 判断domain包含参数中的disk
    - 只对vm中该disk做snapshot，指定domain创建snapshot
  - 空：以current disk作为back file，创建snapshot
- 更新vmds的current等参数
- 更新vmdsn
- 更新config中current

> 删除

- 判断存储池是否active

- 判断snapshot是否已经存在

- 解析config，判断current disk是否存在

- 获取snapshot的`backfile`

- 找到所有以snapshot为backfile的snapshot files

- 如果删除的snapshot files中包含current

  - 判断domain参数（用的命令不同：是否跟domain相关）

    - 非空且active：rebase current的back file为`backfile`

    - 空或非active：rebase current的back file为`backfile`

  - 将current从files中移除

- 删除files文件

- 删除files的vmdsn

> 恢复

## 内部快照相关

> 创建

- 判断domain参数非空，且vm存在
- 判断domain包含参数中的disk
- 创建internal snapshot
- 查询domain的所有internal snapshot，并更新vmds的volume.snaoshots参数

> 删除

- 判断domain参数非空，且vm存在
- 判断domain包含参数中的disk
- 删除internal snapshot
- 查询domain的所有internal snapshot，并更新vmds的volume.snaoshots参数

> 恢复

- 判断domain参数非空，且vm存在
- 判断domain包含参数中的disk
- 恢复到上一个internal snapshot
- 查询domain的所有internal snapshot，并更新vmds的volume.snaoshots参数